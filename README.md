---

## ğŸš€ Most Challenging Problem I Solved Recently

One of the most challenging problems I solved recently was implementing a **Transformer model from scratch**, inspired by the groundbreaking paper *â€œAttention Is All You Need.â€* Instead of using pre-made libraries, I built every major component manuallyâ€”**Scaled Dot-Product Attention**, **Multi-Head Attention**, **Positional Encoding**, **Feed-Forward Blocks**, and **Encoderâ€“Decoder Stacks**.
The biggest challenge came from subtle **numerical stability issues**, especially exploding attention scores and vanishing gradients, which required careful use of scaling factors, masking, and layer normalization. I also optimized the training loop to support large batches and efficient attention masking without losing model accuracy. Debugging these issues taught me how to bridge the gap between theory and real-world implementation.
This project strengthened my understanding of modern deep learning architectures and taught me how to independently translate a research paper into a production-ready model.

ğŸ”— **GitHub Repository:** *<https://github.com/Harsh-Pratap-Singh/Transformer_from_basic>*

---

